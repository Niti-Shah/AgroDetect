{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEYsivYGUoHb",
        "outputId": "1f0c40c9-52fd-484e-e07b-1827b8888100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras_applications in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_applications) (1.22.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_applications) (3.8.0)\n",
            "Found 304 images belonging to 2 classes.\n",
            "Found 308 images belonging to 2 classes.\n",
            "Found 44 images belonging to 2 classes.\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 194s 21s/step - loss: 0.8510 - accuracy: 0.4803 - val_loss: 0.7592 - val_accuracy: 0.5000\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.6898 - accuracy: 0.4934 - val_loss: 0.6597 - val_accuracy: 0.8766\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 9s 954ms/step - loss: 0.6690 - accuracy: 0.5921 - val_loss: 0.6409 - val_accuracy: 0.9123\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 9s 961ms/step - loss: 0.6558 - accuracy: 0.6053 - val_loss: 0.6248 - val_accuracy: 0.7435\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 9s 959ms/step - loss: 0.6149 - accuracy: 0.8553 - val_loss: 0.6092 - val_accuracy: 0.6948\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.6116 - accuracy: 0.6743 - val_loss: 0.5953 - val_accuracy: 0.6461\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 9s 916ms/step - loss: 0.5802 - accuracy: 0.8717 - val_loss: 0.5686 - val_accuracy: 0.8636\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 7s 774ms/step - loss: 0.5748 - accuracy: 0.6941 - val_loss: 0.5748 - val_accuracy: 0.5682\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 9s 900ms/step - loss: 0.5608 - accuracy: 0.7533 - val_loss: 0.5437 - val_accuracy: 0.7468\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.5418 - accuracy: 0.6875 - val_loss: 0.5490 - val_accuracy: 0.6721\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 9s 955ms/step - loss: 0.6081 - accuracy: 0.5921 - val_loss: 0.7365 - val_accuracy: 0.5032\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 9s 960ms/step - loss: 0.6315 - accuracy: 0.6053 - val_loss: 0.5540 - val_accuracy: 0.6266\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.5365 - accuracy: 0.7270 - val_loss: 0.4645 - val_accuracy: 0.9156\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.4631 - accuracy: 0.9408 - val_loss: 0.4634 - val_accuracy: 0.8831\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 9s 967ms/step - loss: 0.4635 - accuracy: 0.8717 - val_loss: 0.4814 - val_accuracy: 0.7792\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 9s 962ms/step - loss: 0.4399 - accuracy: 0.9145 - val_loss: 0.4798 - val_accuracy: 0.7630\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 10s 985ms/step - loss: 0.4442 - accuracy: 0.8816 - val_loss: 0.4197 - val_accuracy: 0.8929\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.4079 - accuracy: 0.9243 - val_loss: 0.3866 - val_accuracy: 0.9091\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 9s 952ms/step - loss: 0.3772 - accuracy: 0.9243 - val_loss: 0.3813 - val_accuracy: 0.9091\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 9s 961ms/step - loss: 0.4033 - accuracy: 0.8651 - val_loss: 0.3607 - val_accuracy: 0.9221\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 9s 965ms/step - loss: 0.3737 - accuracy: 0.9079 - val_loss: 0.3561 - val_accuracy: 0.9058\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.3538 - accuracy: 0.9342 - val_loss: 0.3437 - val_accuracy: 0.9058\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 9s 956ms/step - loss: 0.3558 - accuracy: 0.8914 - val_loss: 0.3574 - val_accuracy: 0.8799\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 9s 965ms/step - loss: 0.3522 - accuracy: 0.9079 - val_loss: 0.3102 - val_accuracy: 0.9188\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 9s 952ms/step - loss: 0.3161 - accuracy: 0.9342 - val_loss: 0.3163 - val_accuracy: 0.9026\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.3047 - accuracy: 0.9375 - val_loss: 0.3015 - val_accuracy: 0.9091\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 9s 963ms/step - loss: 0.2658 - accuracy: 0.9507 - val_loss: 0.2974 - val_accuracy: 0.9026\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 8s 822ms/step - loss: 0.2749 - accuracy: 0.9539 - val_loss: 0.2677 - val_accuracy: 0.9156\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 9s 891ms/step - loss: 0.2826 - accuracy: 0.9243 - val_loss: 0.2736 - val_accuracy: 0.9156\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2487 - accuracy: 0.9539 - val_loss: 0.3065 - val_accuracy: 0.8799\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 9s 939ms/step - loss: 0.2621 - accuracy: 0.9408 - val_loss: 0.2496 - val_accuracy: 0.9253\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 8s 780ms/step - loss: 0.2781 - accuracy: 0.9178 - val_loss: 0.3175 - val_accuracy: 0.8766\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 9s 951ms/step - loss: 0.3243 - accuracy: 0.8553 - val_loss: 0.2422 - val_accuracy: 0.9221\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2112 - accuracy: 0.9605 - val_loss: 0.2352 - val_accuracy: 0.9221\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 9s 948ms/step - loss: 0.2148 - accuracy: 0.9638 - val_loss: 0.2304 - val_accuracy: 0.9221\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 9s 952ms/step - loss: 0.3076 - accuracy: 0.9013 - val_loss: 0.2726 - val_accuracy: 0.8961\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 9s 941ms/step - loss: 0.2393 - accuracy: 0.9243 - val_loss: 0.2673 - val_accuracy: 0.8994\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2668 - accuracy: 0.9046 - val_loss: 0.2373 - val_accuracy: 0.9188\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2567 - accuracy: 0.9046 - val_loss: 0.2199 - val_accuracy: 0.9286\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 9s 897ms/step - loss: 0.2182 - accuracy: 0.9507 - val_loss: 0.2056 - val_accuracy: 0.9253\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 9s 945ms/step - loss: 0.1793 - accuracy: 0.9671 - val_loss: 0.2339 - val_accuracy: 0.9026\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 9s 939ms/step - loss: 0.2055 - accuracy: 0.9441 - val_loss: 0.2326 - val_accuracy: 0.9026\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2329 - accuracy: 0.9507 - val_loss: 0.2099 - val_accuracy: 0.9221\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 9s 895ms/step - loss: 0.2198 - accuracy: 0.9342 - val_loss: 0.1938 - val_accuracy: 0.9318\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 9s 953ms/step - loss: 0.1941 - accuracy: 0.9638 - val_loss: 0.1986 - val_accuracy: 0.9286\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1812 - accuracy: 0.9507 - val_loss: 0.2053 - val_accuracy: 0.9156\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 9s 1000ms/step - loss: 0.1603 - accuracy: 0.9638 - val_loss: 0.1907 - val_accuracy: 0.9318\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 9s 958ms/step - loss: 0.2232 - accuracy: 0.9276 - val_loss: 0.1990 - val_accuracy: 0.9286\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 9s 949ms/step - loss: 0.2175 - accuracy: 0.9079 - val_loss: 0.2533 - val_accuracy: 0.8929\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1929 - accuracy: 0.9342 - val_loss: 0.1826 - val_accuracy: 0.9383\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 9s 910ms/step - loss: 0.2010 - accuracy: 0.9342 - val_loss: 0.1879 - val_accuracy: 0.9253\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 9s 963ms/step - loss: 0.1367 - accuracy: 0.9803 - val_loss: 0.1796 - val_accuracy: 0.9383\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1748 - accuracy: 0.9375 - val_loss: 0.2110 - val_accuracy: 0.9058\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1609 - accuracy: 0.9507 - val_loss: 0.2201 - val_accuracy: 0.9091\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 9s 946ms/step - loss: 0.1921 - accuracy: 0.9474 - val_loss: 0.1787 - val_accuracy: 0.9318\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 9s 950ms/step - loss: 0.2076 - accuracy: 0.9441 - val_loss: 0.1923 - val_accuracy: 0.9286\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 10s 978ms/step - loss: 0.1697 - accuracy: 0.9375 - val_loss: 0.2203 - val_accuracy: 0.9058\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1906 - accuracy: 0.9309 - val_loss: 0.1900 - val_accuracy: 0.9188\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 9s 917ms/step - loss: 0.1748 - accuracy: 0.9408 - val_loss: 0.1754 - val_accuracy: 0.9286\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 9s 946ms/step - loss: 0.1504 - accuracy: 0.9539 - val_loss: 0.2000 - val_accuracy: 0.9253\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2043 - accuracy: 0.9342 - val_loss: 0.1708 - val_accuracy: 0.9383\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1591 - accuracy: 0.9474 - val_loss: 0.1811 - val_accuracy: 0.9383\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 9s 944ms/step - loss: 0.2331 - accuracy: 0.9013 - val_loss: 0.1864 - val_accuracy: 0.9188\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 9s 937ms/step - loss: 0.2574 - accuracy: 0.8914 - val_loss: 0.2177 - val_accuracy: 0.9091\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 9s 945ms/step - loss: 0.2125 - accuracy: 0.9375 - val_loss: 0.2598 - val_accuracy: 0.9026\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2247 - accuracy: 0.9211 - val_loss: 0.1856 - val_accuracy: 0.9286\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 9s 954ms/step - loss: 0.1400 - accuracy: 0.9539 - val_loss: 0.1802 - val_accuracy: 0.9286\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 8s 783ms/step - loss: 0.1733 - accuracy: 0.9605 - val_loss: 0.1683 - val_accuracy: 0.9383\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2334 - accuracy: 0.9145 - val_loss: 0.2870 - val_accuracy: 0.8929\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2570 - accuracy: 0.9211 - val_loss: 0.2242 - val_accuracy: 0.9221\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 8s 854ms/step - loss: 0.1743 - accuracy: 0.9375 - val_loss: 0.2288 - val_accuracy: 0.9221\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 9s 942ms/step - loss: 0.2224 - accuracy: 0.9079 - val_loss: 0.1903 - val_accuracy: 0.9286\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1462 - accuracy: 0.9441 - val_loss: 0.1717 - val_accuracy: 0.9383\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2404 - accuracy: 0.9276 - val_loss: 0.1639 - val_accuracy: 0.9351\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 9s 945ms/step - loss: 0.1667 - accuracy: 0.9375 - val_loss: 0.1643 - val_accuracy: 0.9416\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 7s 771ms/step - loss: 0.1665 - accuracy: 0.9276 - val_loss: 0.1763 - val_accuracy: 0.9253\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 9s 950ms/step - loss: 0.1290 - accuracy: 0.9507 - val_loss: 0.2758 - val_accuracy: 0.8831\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2519 - accuracy: 0.8980 - val_loss: 0.1649 - val_accuracy: 0.9351\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 9s 905ms/step - loss: 0.1410 - accuracy: 0.9474 - val_loss: 0.2016 - val_accuracy: 0.9091\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 9s 948ms/step - loss: 0.1641 - accuracy: 0.9539 - val_loss: 0.2077 - val_accuracy: 0.9091\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 7s 768ms/step - loss: 0.1650 - accuracy: 0.9342 - val_loss: 0.1956 - val_accuracy: 0.9123\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 9s 997ms/step - loss: 0.1264 - accuracy: 0.9671 - val_loss: 0.1619 - val_accuracy: 0.9351\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1655 - accuracy: 0.9474 - val_loss: 0.2259 - val_accuracy: 0.9026\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 9s 942ms/step - loss: 0.1676 - accuracy: 0.9474 - val_loss: 0.1580 - val_accuracy: 0.9351\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 9s 948ms/step - loss: 0.1314 - accuracy: 0.9539 - val_loss: 0.1627 - val_accuracy: 0.9383\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 9s 952ms/step - loss: 0.1488 - accuracy: 0.9474 - val_loss: 0.1987 - val_accuracy: 0.9091\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1331 - accuracy: 0.9704 - val_loss: 0.1564 - val_accuracy: 0.9318\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 9s 945ms/step - loss: 0.1362 - accuracy: 0.9638 - val_loss: 0.1557 - val_accuracy: 0.9351\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 9s 965ms/step - loss: 0.1575 - accuracy: 0.9572 - val_loss: 0.1565 - val_accuracy: 0.9351\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 10s 981ms/step - loss: 0.1219 - accuracy: 0.9539 - val_loss: 0.1878 - val_accuracy: 0.9156\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1413 - accuracy: 0.9539 - val_loss: 0.1785 - val_accuracy: 0.9221\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 9s 943ms/step - loss: 0.1204 - accuracy: 0.9638 - val_loss: 0.1655 - val_accuracy: 0.9286\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 9s 954ms/step - loss: 0.1545 - accuracy: 0.9441 - val_loss: 0.1537 - val_accuracy: 0.9416\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 9s 938ms/step - loss: 0.1506 - accuracy: 0.9572 - val_loss: 0.1525 - val_accuracy: 0.9351\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 9s 936ms/step - loss: 0.1228 - accuracy: 0.9671 - val_loss: 0.1512 - val_accuracy: 0.9448\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 9s 944ms/step - loss: 0.1349 - accuracy: 0.9605 - val_loss: 0.1539 - val_accuracy: 0.9416\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 9s 943ms/step - loss: 0.1606 - accuracy: 0.9671 - val_loss: 0.1551 - val_accuracy: 0.9416\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 9s 956ms/step - loss: 0.1677 - accuracy: 0.9408 - val_loss: 0.1893 - val_accuracy: 0.9318\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1633 - accuracy: 0.9539 - val_loss: 0.1967 - val_accuracy: 0.9286\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 9s 941ms/step - loss: 0.1343 - accuracy: 0.9638 - val_loss: 0.1518 - val_accuracy: 0.9351\n",
            "best Training accuracy: 0.9802631735801697\n",
            "best Validation accuracy: 0.9448052048683167\n",
            "2/2 [==============================] - 6s 5s/step - loss: 0.1322 - accuracy: 0.9318\n",
            "Test accuracy: 0.9318181872367859\n",
            "2/2 [==============================] - 1s 137ms/step\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000130.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000131.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000132.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000133.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000134.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000135.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000136.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000137.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000138.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000139.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000140.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000141.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000142.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000143.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000144.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000145.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000146.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000147.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000148.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000149.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000150.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000151.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000152.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/brown_land/000000000153.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_131.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_132.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_133.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_134.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_135.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_136.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_137.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_138.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_139.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_140.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_141.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_142.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_143.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_144.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_145.jpg contains brown land. Prediction: brown land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_146.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_147.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_148.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_149.jpg contains green land. Prediction: green land.\n",
            "/content/drive/MyDrive/TY-IT/LY-IT/LY Project/main set/test/forest/Pasture_150.jpg contains green land. Prediction: green land.\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_applications\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import image as mpimg\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#flatten reduces dimension of data to a unidimensional array\n",
        "#The Dense layer takes the input tensor, multiplies it with a weight matrix, adds a bias term, (Bias is the difference between predicted values and expected results.)\n",
        "#and applies an activation function to produce the output tensor (tensor=data container, A vector is a 1D tensor, a matrix is a 2D tensor.)\n",
        "\n",
        "# Define the data directories and hyperparameters\n",
        "train_dir = '/content/drive/MyDrive/main set/train'\n",
        "validation_dir = '/content/drive/MyDrive/main set/val'\n",
        "test_dir = '/content/drive/MyDrive/main set/test'\n",
        "img_size = (64, 64)\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "# print(\"Number of training samples:\", len(train_generator.filenames))\n",
        "# print(\"Number of validation samples:\", len(validation_generator.filenames))\n",
        "\n",
        "# Create data generators for the training, validation, and test data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, #to make the values fall in the range [0,1] (normalization)\n",
        "    rotation_range=20, #images will be rotated randomly between -20 to +20 degrees during training.\n",
        "    width_shift_range=0.2, #image can be horizontally shifted by up to 20% of its total width. \n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2, # shearing transformations to the image. (slants the shape of an object)\n",
        "    zoom_range=0.2, #random zooming to the image\n",
        "    horizontal_flip=True, \n",
        "    fill_mode='nearest') #fills gaps in distortion with nearest image\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#flow_from_dir creates an iterator that yields batches of images and their corresponding labels, \n",
        "#which can then be used to train the CNN model.\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', #this sets the value of the vector in each image (0 for secondary and 1 for actual wanted one(1,0))\n",
        "    shuffle=False)\n",
        "\n",
        "\n",
        "#top layer is exculded cuz By excluding the top layer of the pre-trained model, we remove the classification layer and replace it with a new layer that is compatible with the new classification task. This allows us to leverage the feature extraction capabilities of the pre-trained model while adapting the final classification layer to the new problem.\n",
        "# Load the pre-trained ResNet-50 model and freeze its weights\n",
        "resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(64, 64, 3)) #imagenet = model is initialized with pre-trained weights on the ImageNet dataset, top layer of pretrained model shouldnt be included, channels is 3, representing the RGB color channels\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False #these layers will keep their pre-trained weights and will only be used for feature extraction, while the new custom output layer is trained using the updated gradients. (freeze weights bascically)\n",
        "\n",
        "# Add a custom output layer to the ResNet-50 model\n",
        "x = Flatten()(resnet.output) #converts it to a 1D array\n",
        "x = Dense(128, activation=\"relu\")(x) #layer with 128 neurons, relu applies a non-linear transformation to the input data and outputs a new set of values which allows it to learn more complex patterns and relationships in the data.(rectified linear unnit) (Increasing the number of units in the dense layer may improve the performance of the model, but it also increases the number of parameters and the risk of overfitting.)\n",
        "output = Dense(2, activation=\"softmax\")(x) #2 neurons and \"softmax\" activation function which produces a probability distribution over the two possible classes.\n",
        "model = Model(inputs=resnet.input, outputs=output)\n",
        "\n",
        "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with the generators\n",
        "history = model.fit(train_generator, epochs=num_epochs, validation_data=validation_generator)\n",
        "\n",
        "# Output the training and validation accuracy during training\n",
        "print(\"best Training accuracy:\", max(history.history['accuracy']))\n",
        "print(\"best Validation accuracy:\", max(history.history['val_accuracy']))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Save the model\n",
        "model.save('green_brown_land_model.h5') # a data file saved in the Hierarchical Data Format (HDF). It contains multidimensional arrays of scientific data.\n",
        "\n",
        "# Predict the class of the test images\n",
        "predictions = model.predict(test_generator)\n",
        "predicted_classes = np.argmax(predictions, axis=1) #numpy function that takes in the probability predictions and returns the indices of the highest values along axis 1, which represents the class with the highest predicted probability.\n",
        "\n",
        "# Print the predicted class and image label for each test image\n",
        "for i in range(len(predicted_classes)):\n",
        "    img_path = test_generator.filepaths[i]\n",
        "    img = image.load_img(img_path, target_size=img_size)\n",
        "    if predicted_classes[i] == 0:\n",
        "        print(f\"{img_path} contains brown land. Prediction: brown land.\")\n",
        "    else:\n",
        "        print(f\"{img_path} contains green land. Prediction: green land.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfVYiLMtYrCL",
        "outputId": "03ac96d4-3bbd-499b-d6ba-c7b2963bbbe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0VHzKy9LEFy",
        "outputId": "ece35354-67ed-4cdc-acc4-b76e77c16c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras_applications in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_applications) (1.22.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_applications) (3.8.0)\n",
            "Found 310 images belonging to 2 classes.\n",
            "Found 320 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "10/10 [==============================] - 18s 1s/step - loss: 0.7431 - accuracy: 0.5548 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
            "Epoch 2/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.6559 - accuracy: 0.6903 - val_loss: 0.6216 - val_accuracy: 0.8438\n",
            "Epoch 3/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.6188 - accuracy: 0.7000 - val_loss: 0.5883 - val_accuracy: 0.8438\n",
            "Epoch 4/25\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.5615 - accuracy: 0.7903 - val_loss: 0.5638 - val_accuracy: 0.8500\n",
            "Epoch 5/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.5474 - accuracy: 0.7871 - val_loss: 0.5948 - val_accuracy: 0.7125\n",
            "Epoch 6/25\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.5277 - accuracy: 0.7581 - val_loss: 0.5390 - val_accuracy: 0.8344\n",
            "Epoch 7/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.4804 - accuracy: 0.8419 - val_loss: 0.4746 - val_accuracy: 0.8344\n",
            "Epoch 8/25\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.4581 - accuracy: 0.8323 - val_loss: 0.4545 - val_accuracy: 0.8375\n",
            "Epoch 9/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.4218 - accuracy: 0.8516 - val_loss: 0.4665 - val_accuracy: 0.8562\n",
            "Epoch 10/25\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.4146 - accuracy: 0.8419 - val_loss: 0.4164 - val_accuracy: 0.8500\n",
            "Epoch 11/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.3914 - accuracy: 0.8613 - val_loss: 0.3901 - val_accuracy: 0.8625\n",
            "Epoch 12/25\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.4001 - accuracy: 0.8484 - val_loss: 0.3627 - val_accuracy: 0.8562\n",
            "Epoch 13/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.3798 - accuracy: 0.8742 - val_loss: 0.4216 - val_accuracy: 0.8625\n",
            "Epoch 14/25\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.3902 - accuracy: 0.8323 - val_loss: 0.3819 - val_accuracy: 0.8656\n",
            "Epoch 15/25\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.4045 - accuracy: 0.8355 - val_loss: 0.3226 - val_accuracy: 0.8750\n",
            "Epoch 16/25\n",
            "10/10 [==============================] - 17s 2s/step - loss: 0.3397 - accuracy: 0.8839 - val_loss: 0.3360 - val_accuracy: 0.8813\n",
            "Epoch 17/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.3154 - accuracy: 0.9000 - val_loss: 0.2900 - val_accuracy: 0.8875\n",
            "Epoch 18/25\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2562 - accuracy: 0.9258 - val_loss: 0.2768 - val_accuracy: 0.8969\n",
            "Epoch 19/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.2332 - accuracy: 0.9419 - val_loss: 0.3094 - val_accuracy: 0.8875\n",
            "Epoch 20/25\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.2749 - accuracy: 0.9194 - val_loss: 0.3187 - val_accuracy: 0.8813\n",
            "Epoch 21/25\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.3031 - accuracy: 0.8742 - val_loss: 0.3309 - val_accuracy: 0.8813\n",
            "Epoch 22/25\n",
            "10/10 [==============================] - 17s 2s/step - loss: 0.2809 - accuracy: 0.8935 - val_loss: 0.2321 - val_accuracy: 0.9187\n",
            "Epoch 23/25\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2501 - accuracy: 0.9290 - val_loss: 0.2238 - val_accuracy: 0.9187\n",
            "Epoch 24/25\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.2865 - accuracy: 0.8839 - val_loss: 0.2229 - val_accuracy: 0.9125\n",
            "Epoch 25/25\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.2164 - accuracy: 0.9194 - val_loss: 0.2121 - val_accuracy: 0.9312\n",
            "best Training accuracy: 0.9419354796409607\n",
            "best Validation accuracy: 0.9312499761581421\n",
            "4/4 [==============================] - 2s 596ms/step - loss: 0.9727 - accuracy: 0.6700\n",
            "Test accuracy: 0.6700000166893005\n",
            "4/4 [==============================] - 3s 340ms/step\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/39_chip_111.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_0.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_1.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_10.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_11.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_12.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_13.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_14.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_15.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_16.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_17.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_18.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_19.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_2.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_20.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_21.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_22.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_23.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_24.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_25.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_26.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_28.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_29.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_3.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_30.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_31.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_32.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_33.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_34.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_35.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_36.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_37.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_38.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_39.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_4.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_40.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_41.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_42.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_43.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_44.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_45.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_46.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_47.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_48.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_49.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_5.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_6.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_7.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_8.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/black/40_chip_9.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_23.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_24.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_25.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_26.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_27.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_28.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_29.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_30.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_31.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_32.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_33.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_34.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_35.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_36.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_37.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_38.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_39.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_40.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_41.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_42.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_43.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_44.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_45.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_46.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_47.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_48.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_49.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_50.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_51.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_52.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_53.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_54.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_55.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_56.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_57.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_58.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_59.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_60.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_61.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_62.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_63.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_64.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_65.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_66.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_67.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_68.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_69.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_70.jpg contains yellow soil. Prediction: mango.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_71.jpg contains black soil. Prediction: cotton.\n",
            "/content/drive/MyDrive/soil/soil testing/test/yellow/20_chip_72.jpg contains yellow soil. Prediction: mango.\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_applications\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import image as mpimg\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#flatten reduces dimension of data to a unidimensional array\n",
        "#The Dense layer takes the input tensor, multiplies it with a weight matrix, adds a bias term, (Bias is the difference between predicted values and expected results.)\n",
        "#and applies an activation function to produce the output tensor (tensor=data container, A vector is a 1D tensor, a matrix is a 2D tensor.)\n",
        "\n",
        "# Define the data directories and hyperparameters\n",
        "train_dir = '/content/drive/MyDrive/soil/soil testing/Train'\n",
        "validation_dir = '/content/drive/MyDrive/soil/soil testing/val'\n",
        "test_dir = '/content/drive/MyDrive/soil/soil testing/test'\n",
        "img_size = (64, 64)\n",
        "num_epochs = 25\n",
        "batch_size = 32\n",
        "\n",
        "# print(\"Number of training samples:\", len(train_generator.filenames))\n",
        "# print(\"Number of validation samples:\", len(validation_generator.filenames))\n",
        "\n",
        "# Create data generators for the training, validation, and test data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, #to make the values fall in the range [0,1] (normalization)\n",
        "    rotation_range=20, #images will be rotated randomly between -20 to +20 degrees during training.\n",
        "    width_shift_range=0.2, #image can be horizontally shifted by up to 20% of its total width. \n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2, # shearing transformations to the image. (slants the shape of an object)\n",
        "    zoom_range=0.2, #random zooming to the image\n",
        "    horizontal_flip=True, \n",
        "    fill_mode='nearest') #fills gaps in distortion with nearest image\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#flow_from_dir creates an iterator that yields batches of images and their corresponding labels, \n",
        "#which can then be used to train the CNN model.\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', #this sets the value of the vector in each image (0 for secondary and 1 for actual wanted one(1,0))\n",
        "    shuffle=False)\n",
        "\n",
        "\n",
        "#top layer is exculded cuz By excluding the top layer of the pre-trained model, we remove the classification layer and replace it with a new layer that is compatible with the new classification task. This allows us to leverage the feature extraction capabilities of the pre-trained model while adapting the final classification layer to the new problem.\n",
        "# Load the pre-trained ResNet-50 model and freeze its weights\n",
        "resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(64, 64, 3)) #imagenet = model is initialized with pre-trained weights on the ImageNet dataset, top layer of pretrained model shouldnt be included, channels is 3, representing the RGB color channels\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False #these layers will keep their pre-trained weights and will only be used for feature extraction, while the new custom output layer is trained using the updated gradients. (freeze weights bascically)\n",
        "\n",
        "# Add a custom output layer to the ResNet-50 model\n",
        "x = Flatten()(resnet.output) #converts it to a 1D array\n",
        "x = Dense(128, activation=\"relu\")(x) #layer with 128 neurons, relu applies a non-linear transformation to the input data and outputs a new set of values which allows it to learn more complex patterns and relationships in the data.(rectified linear unnit) (Increasing the number of units in the dense layer may improve the performance of the model, but it also increases the number of parameters and the risk of overfitting.)\n",
        "output = Dense(2, activation=\"softmax\")(x) #2 neurons and \"softmax\" activation function which produces a probability distribution over the two possible classes.\n",
        "model = Model(inputs=resnet.input, outputs=output)\n",
        "\n",
        "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with the generators\n",
        "history = model.fit(train_generator, epochs=num_epochs, validation_data=validation_generator)\n",
        "\n",
        "# Output the training and validation accuracy during training\n",
        "print(\"best Training accuracy:\", max(history.history['accuracy']))\n",
        "print(\"best Validation accuracy:\", max(history.history['val_accuracy']))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Save the model\n",
        "model.save('green_brown_land_model.h5') # a data file saved in the Hierarchical Data Format (HDF). It contains multidimensional arrays of scientific data.\n",
        "\n",
        "# Predict the class of the test images\n",
        "predictions = model.predict(test_generator)\n",
        "predicted_classes = np.argmax(predictions, axis=1) #numpy function that takes in the probability predictions and returns the indices of the highest values along axis 1, which represents the class with the highest predicted probability.\n",
        "\n",
        "# Print the predicted class and image label for each test image\n",
        "for i in range(len(predicted_classes)):\n",
        "    img_path = test_generator.filepaths[i]\n",
        "    img = image.load_img(img_path, target_size=img_size)\n",
        "    if predicted_classes[i] == 0:\n",
        "        print(f\"{img_path} contains black soil. Prediction: cotton.\")\n",
        "    else:\n",
        "        print(f\"{img_path} contains yellow soil. Prediction: mango.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}